{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM classification of surface residues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents my work on SVM classification of surface residues of receptors in peptide-protein interactions.\n",
    "The data is read directly from Dana's early work on PeptiDB, analyzing surface residues in that data set using various tools.\n",
    "\n",
    "First, import the necessary Python modules for analysis, primarily SciKit-Learn (sklearn) used for machine learning and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "import pylab as pl\n",
    "import hashlib\n",
    "\n",
    "# caching/serialization libraries\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from datetime import datetime\n",
    "from texttable import Texttable\n",
    "from IPython.core.display import Latex, HTML, display\n",
    "\n",
    "from itertools import combinations, chain\n",
    "from treedict import TreeDict\n",
    "\n",
    "from sklearn import svm, datasets, metrics, cross_validation, preprocessing\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold, LeaveOneLabelOut\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import data\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "THESIS_SRC = '$HOME/lab/msc-thesis/source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.width = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary SVM for feature weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC and feature weight over all features, using cross-validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by defining a K-fold partition of the data, plus a SV classifier. \n",
    "The partition can be either a random partition, a random stratified partition, or by label.\n",
    "\n",
    "In our case, we use 4-fold leave-one-label-out cross-validation. \n",
    "This means we divide the data set of residues into 4 disjoint subsets, such that all the residues of any one receptor are in the same subset. \n",
    "The subsets are roughly similar in size, but cannot be guaranteed to be an equal partition of the data.\n",
    "\n",
    "The partition is accomplished by assigning an integer label to each residue that is the ASCII value of that residue's PDB ID, modulo the number of subsets we want (in our case, 4). \n",
    "The ASCII value of a string is the sum of ASCII numbers of each of its characters. \n",
    "Therefore, all residues of the same receptor will have the same PDB ID, hence the same ASCII value, hence the same label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.utils.load_extensions('gist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_features = [\n",
    "                     'FTMap.cs_size',\n",
    "                     'FTMap.num_nearby_cs',\n",
    "                     'ConSurf.Conservation-score',\n",
    "                     'FPocket.Real-volume-(approximation)',\n",
    "                     'Physicochemical.Polar',\n",
    "                     'Physicochemical.Hydrogen-bonding',\n",
    "                    ]\n",
    "original_feature_set = data.FeatureSet(original_features, original_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_tdict = TreeDict()\n",
    "for context in ('bound', 'unbound'):\n",
    "    for version in ('new', 'new_reduced', 'old',):\n",
    "        data_tdict.update({'.'.join([version, context]) : \n",
    "        data.prepDataSet('%s.data.%s.csv' % (context, version), \n",
    "                         dataset_name='peptalk.{}.{}'.format(context, version),\n",
    "                        )\n",
    "                 })\n",
    "datasets = data_tdict.interactiveTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = datasets.old.bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = d.X.shape\n",
    "\n",
    "# Initialize classifier with crossvalidation\n",
    "k_folds = 4\n",
    "\n",
    "#cv = KFold(len(y), k=k_folds)\n",
    "#cv = StratifiedKFold(y, k=k_folds)\n",
    "\n",
    "hash_to_k = lambda s: (long(hashlib.sha1(s).hexdigest(), 16) % k_folds)\n",
    "pdb_labels = np.array([hash_to_k(s) for s in d.pdbs])\n",
    "cv = LeaveOneLabelOut(labels=pdb_labels)\n",
    "\n",
    "#classifier = svm.SVC(kernel='linear', probability=True, class_weight='auto')\n",
    "classifier = svm.LinearSVC(class_weight='auto', \n",
    "                           dual=True, \n",
    "                           loss='l1', \n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate performance statistics of the classifier, such that for each of the labels we defined above, we:\n",
    "\n",
    "1. Train (or fit) the classifier on other labels, and record the feature weights for the trained model.\n",
    "2. Predict the likelihood of residues from the training set (i.e. other labels) to be binders, and from the test set (the current label).\n",
    "3. Using these predictions, calculate ROC curves for the training and test sets, and record the AUC for each of them.\n",
    "\n",
    "Finally, print all statistics detailed above in a table, where each row represents one label. \n",
    "Additionally, print mean values for all columns (feature weights, AUCs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_weights = np.zeros((len(cv), n_features))\n",
    "test_aucs = np.zeros(len(cv))\n",
    "train_aucs = np.zeros(len(cv))\n",
    "\n",
    "#clf_table = Texttable(max_width=160)\n",
    "#clf_table.set_deco(Texttable.HEADER | Texttable.VLINES)\n",
    "#clf_table.set_cols_dtype(list('t' + 'a'*n_features + 'cc'))\n",
    "#clf_table.set_cols_align(list('l' + 'c'*n_features + 'cc'))\n",
    "#clf_table.set_precision(4)\n",
    "#clf_table.header(['CV subset'] + feature_names + ['AUC (training)', 'AUC (testing)'])\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_tpr_train = 0.0\n",
    "fpr_grid = np.linspace(0, 1, 100)\n",
    "all_tpr = []\n",
    "\n",
    "print \"Calculating CV: \",\n",
    "for i, (train, test) in enumerate(cv):\n",
    "    print i,\n",
    "    classifier.fit(d.X[train,:], d.y[train])\n",
    "    feat_weights[i] = classifier.coef_\n",
    "    \n",
    "    # Test on the training set\n",
    "    tr_conf_scores = classifier.decision_function(d.X[train,:])\n",
    "    tr_fpr, tr_tpr, tr_thresholds = roc_curve(d.y[train], tr_conf_scores)\n",
    "    train_aucs[i] = auc(tr_fpr, tr_tpr)\n",
    "    mean_tpr_train += interp(fpr_grid, tr_fpr, tr_tpr, left=0, right=1)\n",
    "    \n",
    "    #Test on the test set\n",
    "    conf_scores = classifier.decision_function(d.X[test,:])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(d.y[test], conf_scores)\n",
    "    test_aucs[i] = auc(fpr, tpr)\n",
    "    mean_tpr += interp(fpr_grid, fpr, tpr, left=0, right=1)\n",
    "\n",
    "    #clf_table.add_row([\"CV %d\" % i] + feat_weights[i].tolist() + [train_aucs[i], test_aucs[i]])\n",
    "\n",
    "print 'Done.'\n",
    "stats = np.c_[feat_weights, train_aucs, test_aucs]\n",
    "cv_names = [['CV%d'%i] for i in range(len(cv))]\n",
    "#for i in range(len(cv)):\n",
    "#    clf_table.add_row(cv_names[i] + stats[i].tolist())\n",
    "#clf_table.add_row(['Mean'] + stats.mean(axis=0).tolist())\n",
    "\n",
    "mean_tpr /= len(cv)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(fpr_grid, mean_tpr)\n",
    "\n",
    "mean_tpr_train /= len(cv)\n",
    "mean_tpr_train[-1] = 1.0\n",
    "mean_auc_train = auc(fpr_grid, mean_tpr_train)\n",
    "#print \n",
    "#print clf_table.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.precision = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = pd.DataFrame(stats, columns=d.feature_set.features + ['AUC (training)', 'AUC (testing)'], index=['CV%d' % i for i in range(k_folds)])\n",
    "#display(t.append(t.describe()))\n",
    "#display(\n",
    "#        Latex('Feature weights alone:'), \n",
    "#        t.ix[:,:-2].abs().T,\n",
    "#        )\n",
    "\n",
    "tbl_svm_coefs = t.append(t.describe().ix[1:3,:])\n",
    "\n",
    "display(Latex('Entire stats table:'), \n",
    "        tbl_svm_coefs,#.T.abs().sort(columns=['mean',], ascending=False),\n",
    "        )\n",
    "\n",
    "#tbl_svm_coefs.to_csv(os.path.join(THESIS_SRC, '_tables', 'table-svm-coefs.csv'), float_format='%4.2f')\n",
    "#display(t.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves of different classification configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a way to test these configurations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some parameters for `matplotlib` (plotting library):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rcParams = matplotlib.rc_params_from_file(\"/Users/assafl/dev/github/matplotlibrc-huyng/matplotlibrc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rcParams['text.usetex'] = False\n",
    "rcParams['font.size'] = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (12.0, 12.0)\n",
    "rcParams['legend.fontsize'] = 'medium'\n",
    "rcParams['legend.loc'] = 'best'\n",
    "rcParams['lines.linewidth'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test multiple configs in sequence and plot all ROC curves together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testConfigs(configs, saveto=None):\n",
    "    \n",
    "    n_configs = len(configs)\n",
    "    coefs_dict = {}\n",
    "    \n",
    "    #pl.subplot(111)    \n",
    "    for i, c in enumerate(configs):\n",
    "        clf = config.trainClassifier(c)\n",
    "        coefs_dict[c.title] = dict(zip(c.feature_set.features, clf.coef_.tolist()[0]))\n",
    "        conf_scores = clf.decision_function(c.testing.X)\n",
    "        \n",
    "        display(Latex(\"Calculating predictions on feature set: %s\" % c.title))\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(c.testing.y, conf_scores)\n",
    "        fpr_grid = np.linspace(0, 1, 100)\n",
    "        tpr_interp = interp(fpr_grid, fpr, tpr, left=0, right=1)\n",
    "        roc_auc = auc(fpr_grid, tpr_interp)\n",
    "        \n",
    "        is_delta = c.title.find('Delta')!=-1\n",
    "        \n",
    "        pl.plot(\n",
    "                fpr_grid, tpr_interp, \n",
    "                label='%s (AUC = %0.2f)' % (c.title.replace('_',' '), roc_auc),\n",
    "                linestyle='dashed' if is_delta else 'solid',\n",
    "                )\n",
    "    pl.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "    \n",
    "    pl.xlim([-0.05, 1.05])\n",
    "    pl.ylim([-0.05, 1.05])\n",
    "    pl.xlabel('False Positive Rate')\n",
    "    pl.ylabel('True Positive Rate')\n",
    "    #pl.title('Comparison of classifier configurations' + '\\n'+\n",
    "    #        'Mean ROC curves, performance measured on test set')\n",
    "    pl.legend()#bbox_to_anchor=(1, 1), loc=2)\n",
    "    if saveto:\n",
    "        pl.savefig(saveto)\n",
    "    pl.show()\n",
    "    \n",
    "    coefs_df = pd.DataFrame.from_dict(coefs_dict).T\n",
    "    coefs_df.index.names = ['Configurations (by feature set)']\n",
    "    #coefs_df.columns.names = ['Feature weights']\n",
    "    display(coefs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of features against the original six features in our SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compareSingleFeatures(training_set, testing_set, features, delta=False):\n",
    "    tr_csv = training_set.csv_filename\n",
    "    te_csv = testing_set.csv_filename\n",
    "    \n",
    "    full_fs = data.FeatureSet(features, features)\n",
    "    configs = [config.createConfig(full_fs, train=tr_csv, test=te_csv)]\n",
    "    \n",
    "    for feature_singleton in combinations(features, 1):\n",
    "        single_fs = data.FeatureSet(feature_singleton, features)\n",
    "        partial_fs = single_fs.complement() if delta else single_fs\n",
    "        \n",
    "        configs.append(config.createConfig(partial_fs, train=tr_csv, test=te_csv))\n",
    "    \n",
    "    figure_filename = 'roc-configs_{version}_{scope}-{delta}.svg'.format(\n",
    "                             scope='feature',\n",
    "                             version=os.path.basename(tr_csv).split('.')[2],\n",
    "                             delta='delta' if delta else 'single',\n",
    "                             )\n",
    "        \n",
    "    clfs = config.trainConfigClassifiers(configs)\n",
    "    testConfigs(configs, saveto=figure_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the old version of the PeptiDB data, as it was generated by Dana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = datasets.old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate and compare two sets of configs:\n",
    "\n",
    "1. Train each config on *one feature*.\n",
    "2. Train each config on *all features but one* (aka $\\Delta$ configs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rcParams['lines.linewidth'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compareSingleFeatures(d.unbound, d.bound, d.bound.feature_set.features, delta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compareSingleFeatures(d.unbound, d.bound, d.bound.feature_set.features, delta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of protocols against all 48 features calculated recently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compareProtocolFeatures(training_set, testing_set, features, delta=False):\n",
    "    tr_csv = training_set.csv_filename\n",
    "    te_csv = testing_set.csv_filename\n",
    "    \n",
    "    protocols = set(feat.partition('.')[0] for feat in features)\n",
    "    protocol_features = lambda protocol: filter(lambda feat: feat.startswith(protocol), features)\n",
    "    \n",
    "    full_fs = data.FeatureSet(features, features)\n",
    "    configs = [config.createConfig(full_fs, train=tr_csv, test=te_csv)]\n",
    "    \n",
    "    for protocol in sorted(protocols):\n",
    "        single_fs = data.FeatureSet(protocol_features(protocol), features, meta=protocol+' features')\n",
    "        partial_fs = single_fs.complement() if delta else single_fs\n",
    "        \n",
    "        configs.append(config.createConfig(partial_fs, train=tr_csv, test=te_csv, ddg_cutoff=0))\n",
    "        \n",
    "    figure_filename = 'roc-configs_{version}_{scope}-{delta}.svg'.format(\n",
    "                             scope='protocol',\n",
    "                             version=os.path.basename(tr_csv).split('.')[2],\n",
    "                             delta='delta' if delta else 'single',\n",
    "                             )\n",
    "    \n",
    "    clfs = config.trainConfigClassifiers(configs)\n",
    "    testConfigs(configs, saveto=figure_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = datasets.old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compareProtocolFeatures(d.unbound, d.bound, d.bound.feature_set.features, delta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compareProtocolFeatures(d.unbound, d.bound, d.bound.feature_set.features, delta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All possible comparisons in one MEGA loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for compare_func in (compareSingleFeatures, compareProtocolFeatures):\n",
    "    for version in (data_tdict.old, data_tdict.new_reduced):\n",
    "        for delta in (False, True):\n",
    "            compare_func(version.unbound, version.bound, version.bound.feature_set.features, delta=delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old documentation cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from CSV files:\n",
    "\n",
    "Load precomputed data about the samples encoded as tables in CSV format, and scale the data such that each feature has $\\mu = 0$ and $\\sigma^2 = 1$.\n",
    "\n",
    "These data include PDB identifiers, residue numbers, feature values for each of our six features, and lastly $\\Delta\\Delta G$ values for each residue.\n",
    "\n",
    "Note: $\\Delta\\Delta G$ values for residues in the unbound sets are actually calculated for their *bound* counterparts. \n",
    "The correspondence was inferred using a local sequence alignment between bound and unbound receptors, as detailed in the `match-bound-unbound` notebook.\n",
    "\n",
    "### Setup learning:\n",
    "\n",
    "Use the bound feature array as $X$, our feature data, and scale it such that each feature has $\\mu = 0$ and $\\sigma^2 = 1$\n",
    "\n",
    "Load $\\Delta\\Delta G$ values for all residues in the bound set, and define binary classification (`y`) as \"binder\" when a residue has $\\Delta\\Delta G > 0$.\n",
    "\n",
    "Load a list of PDB IDs for each of the samples in the data set, i.e. with length the same as `y`. This is used later in `LeaveOneLabelOut` cross-validation, to make sure that all the residues from any one protein are in the same subset of the samples (i.e. fold)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:: write 4 paragraphs about the SVM section: Q&A\n",
    "\n",
    "check ddg cutoff\n",
    "\n",
    "look at false positives from ConSurf\n",
    "\n",
    "Visualize examples of Dima's table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import peptalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = delta_configs[0].interactiveTree()\n",
    "probs = config.predictClassifier(conf)[:,1]\n",
    "preds = config.predictClassifier(conf, proba=False)\n",
    "ddgs_df = conf.test_set.label_data_df\n",
    "probs_df = pd.DataFrame(data=np.c_[preds, probs], index=conf.test_set.label_data_df.index, columns=['pred','prob']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster_ddg_recall(cluster, ddgs):\n",
    "    return ddgs[cluster].sum() / ddgs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def test_clustering(conf, const_size=None):\n",
    "    probs = config.predictClassifier(conf)[:,1]\n",
    "    preds = config.predictClassifier(conf, proba=False)\n",
    "    ddgs_df = conf.test_set.label_data_df\n",
    "    probs_df = pd.DataFrame(data=np.c_[preds, probs], index=ddgs_df.index, columns=['pred','prob']) \n",
    "    \n",
    "    cluster_results = {}\n",
    "    for pdbid, df in probs_df.groupby(level=0):\n",
    "        #print pdbid,\n",
    "        svm_result = peptalk.PeptalkResult(pdbid=pdbid, preds=df.pred, confidence=df.prob)\n",
    "        \n",
    "        dbscan_cls = svm_result.cluster_dbscan()\n",
    "        #print pdbid, dbscan_cls\n",
    "        cl = dbscan_cls[0] if dbscan_cls else []\n",
    "        \n",
    "        prob_cl_size = const_size if const_size else len(cl)\n",
    "        prob_cl_df = df.sort(column='prob', ascending=False).head(prob_cl_size).sort()\n",
    "        prob_cl = prob_cl_df['prob'].index.get_level_values(1).tolist()\n",
    "        #print cl, prob_cl\n",
    "        ddgs = ddgs_df.ix[pdbid]\n",
    "        cluster_results[pdbid] = (cluster_ddg_recall(cl, ddgs), cluster_ddg_recall(prob_cl, ddgs))\n",
    "    print \"\"\n",
    "    return cluster_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def summarize(conf):\n",
    "    cluster_results = test_clustering(conf, const_size=None)\n",
    "    d = pd.DataFrame(cluster_results.values(), index=cluster_results.keys(), columns=['clustered','nonclustered'])\n",
    "    return (d > .5).sum() / float(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summ = pd.concat([summarize(c) for c in single_protocol_configs], keys=[c.title for c in single_protocol_configs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summ.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = pd.DataFrame(cluster_results.values(), index=cluster_results.keys(), columns=['len','clustered','nonclustered'])\n",
    "d.clustered - d.nonclustered\n",
    "d.ix[(d.clustered > d.nonclustered) & (d.clustered!=0)]\n",
    "#d.ix[d.len <2]\n",
    "pl.hist([d[col] for col in d.columns[1:]], \n",
    "        label=d.columns[1:].tolist(),\n",
    "        alpha=.8,\n",
    "        bins=linspace(0, 1, num=11, endpoint=True),\n",
    "        )\n",
    "pl.legend(loc='upper right')\n",
    "pl.xticks(linspace(0, 1, num=11, endpoint=True))\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV, RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -1 unboundMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=classifier, n_features_to_select=3, step=1)\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print rfe.support_\n",
    "print rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfecv = RFECV(classifier, step=1, cv=StratifiedKFold(y_train, 3),\n",
    "                loss_func=metrics.zero_one)\n",
    "rfecv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Optimal number of features : %d\" % rfecv.n_features_\n",
    "print \"Support:\", rfecv.support_\n",
    "print \"Feature ranking:\", rfecv.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure()\n",
    "pl.xlabel(\"Number of features selected\")\n",
    "pl.ylabel(\"Cross validation score (nb of misclassifications)\")\n",
    "pl.plot(xrange(1, len(rfecv.cv_scores_) + 1), rfecv.cv_scores_)\n",
    "pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
